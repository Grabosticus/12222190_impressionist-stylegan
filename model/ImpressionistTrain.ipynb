{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ad6927a",
   "metadata": {},
   "source": [
    "# Impressionist StyleGAN - Training Loop\n",
    "Run the cells below to train your own StyleGAN on the dataset of impressionist artworks. Make sure have a directory `impressionist` that contains the images from the dataset (you can find the dataset in the GitHub Release called `Impressionist Artworks v1.0`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c30259c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexa\\anaconda3\\envs\\impressionist-stylegan\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import ImpressionistDataset as dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import generator\n",
    "import discriminator\n",
    "import globals \n",
    "import math\n",
    "import utils\n",
    "from tqdm import tqdm\n",
    "import importlib\n",
    "from utils_generator import applyExponentialMovingAverage, g_loss_non_saturating\n",
    "from utils_discriminator import d_loss_non_saturating_r1\n",
    "from ADA import ADA\n",
    "from torch.utils.data import ConcatDataset\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a8c9296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'ImpressionistDataset' from 'c:\\\\Users\\\\alexa\\\\Desktop\\\\AppliedDeepLearning\\\\model\\\\ImpressionistDataset.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(generator)\n",
    "importlib.reload(discriminator)\n",
    "importlib.reload(globals)\n",
    "importlib.reload(utils)\n",
    "importlib.reload(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecd7c7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_ind = 0\n",
    "datasets = {\n",
    "    4: dataset.ImpressionistDataset(resolution=4, cluster_ind=cluster_ind),\n",
    "    8: dataset.ImpressionistDataset(resolution=8, cluster_ind=cluster_ind),\n",
    "    16: dataset.ImpressionistDataset(resolution=16, cluster_ind=cluster_ind),\n",
    "    32: dataset.ImpressionistDataset(resolution=32, cluster_ind=cluster_ind),\n",
    "    64: dataset.ImpressionistDataset(resolution=64, cluster_ind=cluster_ind),\n",
    "    128: dataset.ImpressionistDataset(resolution=128, cluster_ind=cluster_ind)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c446b854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESOLUTION 4x4:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/147 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REAL SCORES: tensor([[-0.2108],\n",
      "        [ 0.0156],\n",
      "        [ 0.1335],\n",
      "        [ 0.3583],\n",
      "        [ 0.5205],\n",
      "        [ 0.0171],\n",
      "        [ 0.2762],\n",
      "        [ 0.2478],\n",
      "        [-0.3006],\n",
      "        [ 0.1438]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "FAKE SCORES: tensor([[-0.2274],\n",
      "        [ 0.6772],\n",
      "        [ 0.7035],\n",
      "        [ 0.4699],\n",
      "        [ 0.7292],\n",
      "        [ 0.1080],\n",
      "        [-0.4680],\n",
      "        [ 0.2875],\n",
      "        [ 0.0337],\n",
      "        [ 0.2886]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "DISCRIMINATOR_LOSS: 1.545292854309082\n",
      "ADA rt: 0.4609, p: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 147/147 [00:54<00:00,  2.70it/s]\n",
      " 72%|███████▏  | 106/147 [00:43<00:09,  4.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REAL SCORES: tensor([[ 0.6894],\n",
      "        [ 0.3892],\n",
      "        [ 0.4608],\n",
      "        [ 0.5563],\n",
      "        [-0.0143],\n",
      "        [ 0.5273],\n",
      "        [ 0.5628],\n",
      "        [ 0.0198],\n",
      "        [ 0.5393],\n",
      "        [ 0.5047]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "FAKE SCORES: tensor([[-0.6370],\n",
      "        [-0.8745],\n",
      "        [-0.8191],\n",
      "        [-0.5500],\n",
      "        [-0.6001],\n",
      "        [-0.7074],\n",
      "        [-0.8241],\n",
      "        [-0.6865],\n",
      "        [-0.7831],\n",
      "        [-0.7753]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "DISCRIMINATOR_LOSS: 0.9045981168746948\n",
      "ADA rt: 0.9531, p: 0.0202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 147/147 [00:53<00:00,  2.72it/s]\n",
      " 43%|████▎     | 63/147 [00:32<00:17,  4.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REAL SCORES: tensor([[ 0.2496],\n",
      "        [ 0.1407],\n",
      "        [ 0.0092],\n",
      "        [ 0.7148],\n",
      "        [ 0.6358],\n",
      "        [ 0.5729],\n",
      "        [-0.0145],\n",
      "        [ 0.5311],\n",
      "        [ 0.4615],\n",
      "        [ 0.5736]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "FAKE SCORES: tensor([[-0.5944],\n",
      "        [-0.5589],\n",
      "        [-0.5212],\n",
      "        [-0.6699],\n",
      "        [-0.7701],\n",
      "        [-0.9024],\n",
      "        [-0.8811],\n",
      "        [-0.5450],\n",
      "        [-0.5243],\n",
      "        [-0.7699]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "DISCRIMINATOR_LOSS: 0.9028184413909912\n",
      "ADA rt: 0.9219, p: 0.0289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 147/147 [03:11<00:00,  1.31s/it]\n",
      " 16%|█▌        | 23/147 [00:22<00:28,  4.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REAL SCORES: tensor([[0.6046],\n",
      "        [0.5317],\n",
      "        [0.3971],\n",
      "        [0.5679],\n",
      "        [0.3104],\n",
      "        [0.5240],\n",
      "        [0.4250],\n",
      "        [0.3817],\n",
      "        [0.4884],\n",
      "        [0.4139]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "FAKE SCORES: tensor([[-0.2257],\n",
      "        [-0.1814],\n",
      "        [-0.2335],\n",
      "        [-0.3167],\n",
      "        [-0.4788],\n",
      "        [-0.6122],\n",
      "        [-0.5355],\n",
      "        [-0.2731],\n",
      "        [-0.4559],\n",
      "        [-0.4437]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "DISCRIMINATOR_LOSS: 1.0638139247894287\n",
      "ADA rt: 0.9766, p: 0.0375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 147/147 [00:51<00:00,  2.85it/s]\n",
      " 86%|████████▋ | 127/147 [00:49<00:04,  4.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REAL SCORES: tensor([[ 0.1138],\n",
      "        [ 0.2695],\n",
      "        [ 0.2428],\n",
      "        [-0.0033],\n",
      "        [ 0.2473],\n",
      "        [ 0.2405],\n",
      "        [ 0.1553],\n",
      "        [ 0.2168],\n",
      "        [ 0.1855],\n",
      "        [ 0.3418]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "FAKE SCORES: tensor([[-0.0551],\n",
      "        [-0.0931],\n",
      "        [ 0.0138],\n",
      "        [-0.1597],\n",
      "        [ 0.0835],\n",
      "        [ 0.2239],\n",
      "        [ 0.0962],\n",
      "        [-0.0333],\n",
      "        [ 0.0457],\n",
      "        [ 0.0966]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "DISCRIMINATOR_LOSS: 1.3862230777740479\n",
      "ADA rt: 0.7578, p: 0.0525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 147/147 [00:56<00:00,  2.61it/s]\n",
      " 33%|███▎      | 49/147 [01:11<02:22,  1.45s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 110\u001b[0m\n\u001b[0;32m    108\u001b[0m     count_until_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50000\u001b[39m\n\u001b[0;32m    109\u001b[0m     percent_this_phase \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\u001b[38;5;241m*\u001b[39mimgs_this_phase \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mglobals\u001b[39m\u001b[38;5;241m.\u001b[39mIMAGES_PER_RESOLUTION[res])\n\u001b[1;32m--> 110\u001b[0m     fid_score \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mG_EMA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mres\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpercent_this_phase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m count_until_grid \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    113\u001b[0m     utils\u001b[38;5;241m.\u001b[39mgenerate_grid_image(G, fid_score[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mG\u001b[39m\u001b[38;5;124m\"\u001b[39m], res, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_imgs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\alexa\\Desktop\\AppliedDeepLearning\\model\\utils.py:87\u001b[0m, in \u001b[0;36mcompute_fid\u001b[1;34m(G, G_EMA, dataset, res, percent_this_phase, fid, output_file, max_imgs)\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     86\u001b[0m real_images \u001b[38;5;241m=\u001b[39m transform(batch) \u001b[38;5;66;03m# FID needs images to be 299x299\u001b[39;00m\n\u001b[1;32m---> 87\u001b[0m real_images \u001b[38;5;241m=\u001b[39m \u001b[43mreal_images\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# FID needs images to be in [0,255]\u001b[39;00m\n\u001b[0;32m     89\u001b[0m real_images \u001b[38;5;241m=\u001b[39m (real_images \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2.0\u001b[39m \n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "ada = ADA()\n",
    "fid = FrechetInceptionDistance(feature=2048).to(globals.DEVICE)\n",
    "\n",
    "G = generator.Generator()\n",
    "D = discriminator.Discriminator()\n",
    "G.to(globals.DEVICE), D.to(globals.DEVICE)\n",
    "\n",
    "# we initialize our EMA Generator. We don't need gradients for it.\n",
    "G_EMA = generator.Generator()\n",
    "G_EMA.load_state_dict(G.state_dict())\n",
    "G_EMA.train(False)\n",
    "G_EMA.to(globals.DEVICE)\n",
    "\n",
    "for param in G_EMA.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "mapping_params, generator_params = utils.get_generator_params(G)\n",
    "\n",
    "adam_g = torch.optim.AdamW([\n",
    "    {'params': mapping_params, 'lr': globals.LR_MAPPING_NETWORK, 'name': 'mapping'},\n",
    "    {'params': generator_params, 'lr': globals.LR_MODEL, 'name': 'generator'}\n",
    "], betas=(globals.ADAM_BETA1, globals.ADAM_BETA2))\n",
    "\n",
    "\n",
    "adam_d = torch.optim.AdamW(D.parameters(), lr=globals.LR_MODEL, betas=(globals.ADAM_BETA1, globals.ADAM_BETA2))\n",
    "\n",
    "res_list = [2**i for i in range(2, int(math.log2(globals.MAX_RES))+1)]\n",
    "\n",
    "global_img_count = 0\n",
    "for res in res_list:\n",
    "\n",
    "    # we update the learning rate for each resolution\n",
    "    g_lr = globals.LR_MODEL_PER_RES[res]\n",
    "    d_lr = globals.LR_MODEL_PER_RES[res]\n",
    "    mapping_lr = globals.LR_MAPPING_NETWORK_PER_RES[res]\n",
    "\n",
    "    for param_group in adam_g.param_groups:\n",
    "        if param_group.get('name') == 'mapping':\n",
    "            param_group['lr'] = mapping_lr\n",
    "        else:\n",
    "            param_group['lr'] = g_lr\n",
    "\n",
    "    for param_group in adam_d.param_groups:\n",
    "        param_group[\"lr\"] = d_lr\n",
    "\n",
    "    print(f\"RESOLUTION {res}x{res}:\")\n",
    "    if res > 4:\n",
    "        G.fade_in(res)\n",
    "        G_EMA.fade_in(res)\n",
    "        D.fade_in(res)\n",
    "    \n",
    "    repeated_dataset = ConcatDataset([datasets[res]] * 3)\n",
    "    loader = torch.utils.data.DataLoader(repeated_dataset, batch_size=globals.BATCH_SIZES_PER_RES[res], shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "    fade_in_imgs = int(globals.IMAGES_PER_RESOLUTION[res] * globals.FADE_IN_PERCENTAGE)\n",
    "    count_until_fid = 50000\n",
    "    count_until_grid = 100000\n",
    "    imgs_this_phase = 0\n",
    "    discriminator_steps = 0\n",
    "\n",
    "    while imgs_this_phase < globals.IMAGES_PER_RESOLUTION[res]:\n",
    "        for real in tqdm(loader):\n",
    "            real = real.to(globals.DEVICE)\n",
    "\n",
    "            batch_size = real.size(0)\n",
    "\n",
    "            if imgs_this_phase < fade_in_imgs:\n",
    "                layer_opacity = min(1.0, imgs_this_phase / max(1, fade_in_imgs))\n",
    "                G.set_layer_opacity(layer_opacity)\n",
    "                G_EMA.set_layer_opacity(layer_opacity)\n",
    "                D.set_layer_opacity(layer_opacity)\n",
    "            else:\n",
    "                G.set_layer_opacity(1.0)\n",
    "                G_EMA.set_layer_opacity(1.0)\n",
    "                D.set_layer_opacity(1.0)\n",
    "\n",
    "            # only one D step for logistic loss with R1\n",
    "            for i in range(globals.DISCRIMINATOR_STEPS):\n",
    "                # Discriminator step\n",
    "                adam_d.zero_grad(set_to_none=True)\n",
    "                discriminator_steps += 1\n",
    "                z = torch.randn(batch_size, globals.Z_DIM, device=globals.DEVICE)\n",
    "                with torch.no_grad():\n",
    "                    fake = G(z)\n",
    "\n",
    "                log = imgs_this_phase % len(loader) == 0 and i == 0\n",
    "                D_loss = d_loss_non_saturating_r1(D, real, fake.detach(), discriminator_steps, ada, log=log)\n",
    "                D_loss.backward()\n",
    "                adam_d.step()\n",
    "\n",
    "            # Generator step\n",
    "            z = torch.randn(batch_size, globals.Z_DIM, device=globals.DEVICE)\n",
    "            adam_g.zero_grad(set_to_none=True)\n",
    "\n",
    "            fake = G(z)\n",
    "            G_loss = g_loss_non_saturating(D, fake, ada)\n",
    "            G_loss.backward()\n",
    "            adam_g.step()\n",
    "            applyExponentialMovingAverage(G, G_EMA)\n",
    "\n",
    "            imgs_this_phase += batch_size\n",
    "            global_img_count += batch_size\n",
    "            count_until_fid -= batch_size\n",
    "            count_until_grid -= batch_size\n",
    "            if count_until_fid <= 0:\n",
    "                count_until_fid = 50000\n",
    "                percent_this_phase = 100*imgs_this_phase / (globals.IMAGES_PER_RESOLUTION[res])\n",
    "                fid_score = utils.compute_fid(G, G_EMA, datasets[res], res, percent_this_phase, fid)\n",
    "\n",
    "            if count_until_grid <= 0:\n",
    "                utils.generate_grid_image(G, fid_score[\"G\"], res, \"training_imgs\")\n",
    "                count_until_grid = 100000\n",
    "\n",
    "    G.set_layer_opacity(1.0)\n",
    "    G_EMA.set_layer_opacity(1.0)\n",
    "    D.set_layer_opacity(1.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234a7943",
   "metadata": {},
   "source": [
    "# Save Model Weights\n",
    "\n",
    "Run the below cell to save the current state of the model i.e. the Generator/Discriminator weights and the Optimizer states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6804362",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "        'G_state_dict': G.state_dict(),\n",
    "        'D_state_dict': D.state_dict(),\n",
    "        'G_EMA_state_dict': G_EMA.state_dict(),\n",
    "        'G_optimizer': adam_g.state_dict(),\n",
    "        'D_optimizer': adam_d.state_dict()\n",
    "}, \"weights/ada_stylegan_64_more_channels.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3395f76e",
   "metadata": {},
   "source": [
    "# Load Saved EMA Generator\n",
    "\n",
    "Run the below cell to load a pretrained EMA Generator. You can find the weights file of my StyleGAN in GitHub in the `Impressionist Artworks v1.0` Release.\n",
    "After is has been loaded, we generate some images with it.\n",
    "The result will be saved in the directory `final_model_imgs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b059967",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_12168\\1582446206.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n"
     ]
    }
   ],
   "source": [
    "G_EMA = generator.Generator().to(globals.DEVICE)\n",
    "FINAL_MODEL_RESOLUTION = 64\n",
    "\n",
    "checkpoint = torch.load(\"weights/ada_stylegan_64_more_channels.pth\")\n",
    "G_EMA.load_state_dict(checkpoint['G_EMA_state_dict'])\n",
    "G_EMA.fade_in(FINAL_MODEL_RESOLUTION)\n",
    "G_EMA.set_layer_opacity(1.0)\n",
    "\n",
    "FINAL_MODEL_FID_SCORE = 40.38\n",
    "utils.generate_grid_image(G_EMA, FINAL_MODEL_FID_SCORE, FINAL_MODEL_RESOLUTION, \"final_model_imgs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "impressionist-stylegan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

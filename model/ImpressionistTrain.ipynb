{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ad6927a",
   "metadata": {},
   "source": [
    "# Impressionist StyleGAN - Training Loop\n",
    "Run the cells below to train your own StyleGAN on the dataset of impressionist artworks. Make sure have a directory `impressionist` that contains the images from the dataset (you can find the dataset in the GitHub Release called `Impressionist Artworks v1.0`).\n",
    "The training loop calculates FID scores every 50_000 images shown to the discriminator and saves a grid of generated images every 100_000 images shown to the discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c30259c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import ImpressionistDataset as dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import generator\n",
    "import discriminator\n",
    "import globals \n",
    "import math\n",
    "import utils\n",
    "from tqdm import tqdm\n",
    "import importlib\n",
    "from utils_generator import apply_exponential_moving_average, g_loss_non_saturating\n",
    "from utils_discriminator import d_loss_non_saturating_r1\n",
    "from ADA import ADA\n",
    "from torch.utils.data import ConcatDataset\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8c9296",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(generator)\n",
    "importlib.reload(discriminator)\n",
    "importlib.reload(globals)\n",
    "importlib.reload(utils)\n",
    "importlib.reload(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd7c7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_ind = 0\n",
    "datasets = {\n",
    "    4: dataset.ImpressionistDataset(resolution=4, cluster_ind=cluster_ind),\n",
    "    8: dataset.ImpressionistDataset(resolution=8, cluster_ind=cluster_ind),\n",
    "    16: dataset.ImpressionistDataset(resolution=16, cluster_ind=cluster_ind),\n",
    "    32: dataset.ImpressionistDataset(resolution=32, cluster_ind=cluster_ind),\n",
    "    64: dataset.ImpressionistDataset(resolution=64, cluster_ind=cluster_ind),\n",
    "    128: dataset.ImpressionistDataset(resolution=128, cluster_ind=cluster_ind),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c446b854",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "ada = ADA()\n",
    "fid = FrechetInceptionDistance(feature=2048).to(globals.DEVICE)\n",
    "\n",
    "G = generator.Generator()\n",
    "D = discriminator.Discriminator()\n",
    "G.to(globals.DEVICE), D.to(globals.DEVICE)\n",
    "\n",
    "# we initialize our EMA Generator. We don't need gradients for it.\n",
    "G_EMA = generator.Generator()\n",
    "G_EMA.load_state_dict(G.state_dict())\n",
    "G_EMA.train(False)\n",
    "G_EMA.to(globals.DEVICE)\n",
    "\n",
    "for param in G_EMA.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "mapping_params, generator_params = utils.get_generator_params(G)\n",
    "\n",
    "adam_g = torch.optim.AdamW(\n",
    "    [\n",
    "        {\"params\": mapping_params, \"lr\": globals.LR_MAPPING_NETWORK, \"name\": \"mapping\"},\n",
    "        {\"params\": generator_params, \"lr\": globals.LR_MODEL, \"name\": \"generator\"},\n",
    "    ],\n",
    "    betas=(globals.ADAM_BETA1, globals.ADAM_BETA2),\n",
    ")\n",
    "\n",
    "\n",
    "adam_d = torch.optim.AdamW(\n",
    "    D.parameters(), lr=globals.LR_MODEL, betas=(globals.ADAM_BETA1, globals.ADAM_BETA2)\n",
    ")\n",
    "\n",
    "res_list = [2**i for i in range(2, int(math.log2(globals.MAX_RES)) + 1)]\n",
    "\n",
    "global_img_count = 0\n",
    "for res in res_list:\n",
    "\n",
    "    # we update the learning rate for each resolution\n",
    "    g_lr = globals.LR_MODEL_PER_RES[res]\n",
    "    d_lr = globals.LR_MODEL_PER_RES[res]\n",
    "    mapping_lr = globals.LR_MAPPING_NETWORK_PER_RES[res]\n",
    "\n",
    "    for param_group in adam_g.param_groups:\n",
    "        if param_group.get(\"name\") == \"mapping\":\n",
    "            param_group[\"lr\"] = mapping_lr\n",
    "        else:\n",
    "            param_group[\"lr\"] = g_lr\n",
    "\n",
    "    for param_group in adam_d.param_groups:\n",
    "        param_group[\"lr\"] = d_lr\n",
    "\n",
    "    print(f\"RESOLUTION {res}x{res}:\")\n",
    "    if res > 4:\n",
    "        G.fade_in(res)\n",
    "        G_EMA.fade_in(res)\n",
    "        D.fade_in(res)\n",
    "\n",
    "    repeated_dataset = ConcatDataset([datasets[res]] * 3)\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        repeated_dataset,\n",
    "        batch_size=globals.BATCH_SIZES_PER_RES[res],\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    fade_in_imgs = int(globals.IMAGES_PER_RESOLUTION[res] * globals.FADE_IN_PERCENTAGE)\n",
    "    count_until_fid = 50000\n",
    "    count_until_grid = 100000\n",
    "    imgs_this_phase = 0\n",
    "    discriminator_steps = 0\n",
    "\n",
    "    while imgs_this_phase < globals.IMAGES_PER_RESOLUTION[res]:\n",
    "        print(\n",
    "            f\"Images this phase: {imgs_this_phase}/{globals.IMAGES_PER_RESOLUTION[res]}\"\n",
    "        )\n",
    "        for real in tqdm(loader):\n",
    "            real = real.to(globals.DEVICE)\n",
    "\n",
    "            batch_size = real.size(0)\n",
    "\n",
    "            if imgs_this_phase < fade_in_imgs:\n",
    "                layer_opacity = min(1.0, imgs_this_phase / max(1, fade_in_imgs))\n",
    "                G.set_layer_opacity(layer_opacity)\n",
    "                G_EMA.set_layer_opacity(layer_opacity)\n",
    "                D.set_layer_opacity(layer_opacity)\n",
    "            else:\n",
    "                G.set_layer_opacity(1.0)\n",
    "                G_EMA.set_layer_opacity(1.0)\n",
    "                D.set_layer_opacity(1.0)\n",
    "\n",
    "            # only one D step for logistic loss with R1\n",
    "            for i in range(globals.DISCRIMINATOR_STEPS):\n",
    "                # Discriminator step\n",
    "                adam_d.zero_grad(set_to_none=True)\n",
    "                discriminator_steps += 1\n",
    "                z = torch.randn(batch_size, globals.Z_DIM, device=globals.DEVICE)\n",
    "                with torch.no_grad():\n",
    "                    fake = G(z)\n",
    "\n",
    "                D_loss = d_loss_non_saturating_r1(\n",
    "                    D, real, fake.detach(), discriminator_steps, ada\n",
    "                )\n",
    "                D_loss.backward()\n",
    "                adam_d.step()\n",
    "\n",
    "            # Generator step\n",
    "            z = torch.randn(batch_size, globals.Z_DIM, device=globals.DEVICE)\n",
    "            adam_g.zero_grad(set_to_none=True)\n",
    "\n",
    "            fake = G(z)\n",
    "            G_loss = g_loss_non_saturating(D, fake, ada)\n",
    "            G_loss.backward()\n",
    "            adam_g.step()\n",
    "            apply_exponential_moving_average(G, G_EMA)\n",
    "\n",
    "            imgs_this_phase += batch_size\n",
    "            global_img_count += batch_size\n",
    "            count_until_fid -= batch_size\n",
    "            count_until_grid -= batch_size\n",
    "            if count_until_fid <= 0:\n",
    "                count_until_fid = 50000\n",
    "                percent_this_phase = (\n",
    "                    100 * imgs_this_phase / (globals.IMAGES_PER_RESOLUTION[res])\n",
    "                )\n",
    "                print(\"Calculating FID...\")\n",
    "                fid_score = utils.compute_fid(\n",
    "                    G, G_EMA, datasets[res], res, percent_this_phase, fid\n",
    "                )\n",
    "\n",
    "            if count_until_grid <= 0:\n",
    "                print(\"Generating Image Grid...\")\n",
    "                utils.generate_grid_image(G, fid_score[\"G\"], res, \"training_imgs\")\n",
    "                count_until_grid = 100000\n",
    "\n",
    "    G.set_layer_opacity(1.0)\n",
    "    G_EMA.set_layer_opacity(1.0)\n",
    "    D.set_layer_opacity(1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234a7943",
   "metadata": {},
   "source": [
    "# Save Model Weights\n",
    "\n",
    "Run the below cell to save the current state of the model i.e. the Generator/Discriminator weights and the Optimizer states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6804362",
   "metadata": {},
   "outputs": [],
   "source": [
    "YOUR_MODEL_NAME = \"Write_a_name_for_your_model_here\"\n",
    "\n",
    "os.makedirs(\"weights\", exist_ok=True)\n",
    "torch.save(\n",
    "    {\n",
    "        \"G_state_dict\": G.state_dict(),\n",
    "        \"D_state_dict\": D.state_dict(),\n",
    "        \"G_EMA_state_dict\": G_EMA.state_dict(),\n",
    "        \"G_optimizer\": adam_g.state_dict(),\n",
    "        \"D_optimizer\": adam_d.state_dict(),\n",
    "    },\n",
    "    f\"weights/{YOUR_MODEL_NAME}.pth\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3395f76e",
   "metadata": {},
   "source": [
    "# Load Saved EMA Generator\n",
    "\n",
    "Run the below cell to load a pretrained EMA Generator. You can find the weights file of my StyleGAN in GitHub in the `Impressionist Artworks v1.0` Release.\n",
    "After it has been loaded, we generate some images with it.\n",
    "The result will be saved in the directory `final_model_imgs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b059967",
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_MODEL_RESOLUTION = 64\n",
    "FINAL_MODEL_FID_SCORE = 40.38\n",
    "\n",
    "G_EMA = generator.Generator().to(globals.DEVICE)\n",
    "checkpoint = torch.load(\"weights/ada_stylegan_64_more_channels.pth\")\n",
    "G_EMA.load_state_dict(checkpoint[\"G_EMA_state_dict\"])\n",
    "G_EMA.fade_in(FINAL_MODEL_RESOLUTION)\n",
    "G_EMA.set_layer_opacity(1.0)\n",
    "\n",
    "utils.generate_grid_image(\n",
    "    G_EMA, FINAL_MODEL_FID_SCORE, FINAL_MODEL_RESOLUTION, \"final_model_imgs\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "impressionist-stylegan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

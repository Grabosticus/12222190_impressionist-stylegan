{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c80a5cd",
   "metadata": {},
   "source": [
    "# MNIST Training Notebook\n",
    "\n",
    "Since my dataset of impressionist artworks is very diverse, and therefore requires very long training (>24h) to produce good images, I created this notebook where you can test the StyleGAN on a simpler dataset (MNIST). I used the same hyperparameters for this (simpler) dataset as for the impressionist dataset, but decreased the training time and omitted time-consuming calculations (such as FID), which are used for the impressionist dataset.\n",
    "After every epoch a generated image will be shown, so you can see how the model gets better during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa7a8639",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexa\\anaconda3\\envs\\impressionist-stylegan\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import MNISTDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import generator\n",
    "import discriminator\n",
    "import globals\n",
    "import math\n",
    "import utils\n",
    "from tqdm import tqdm\n",
    "import importlib\n",
    "import numpy as np\n",
    "from utils_generator import g_loss_non_saturating, apply_exponential_moving_average\n",
    "from utils_discriminator import d_loss_non_saturating_r1\n",
    "from ADA import ADA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73658e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(generator)\n",
    "importlib.reload(discriminator)\n",
    "importlib.reload(MNISTDataset)\n",
    "importlib.reload(globals)\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f20823a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    4: MNISTDataset.MNISTDataset(resolution=4),\n",
    "    8: MNISTDataset.MNISTDataset(resolution=8),\n",
    "    16: MNISTDataset.MNISTDataset(resolution=16),\n",
    "    32: MNISTDataset.MNISTDataset(resolution=32)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56fa65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "G = generator.Generator()\n",
    "D = discriminator.Discriminator()\n",
    "G.to(globals.DEVICE), D.to(globals.DEVICE)\n",
    "\n",
    "G_EMA = generator.Generator()\n",
    "G_EMA.load_state_dict(G.state_dict())\n",
    "G_EMA.train(False)\n",
    "G_EMA.to(globals.DEVICE)\n",
    "\n",
    "for param in G_EMA.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "ada = ADA()\n",
    "\n",
    "mapping_params, generator_params = utils.get_generator_params(G)\n",
    "\n",
    "adam_g = torch.optim.Adam([\n",
    "    {'params': mapping_params, 'lr': globals.LR_MAPPING_NETWORK},\n",
    "    {'params': generator_params, 'lr': globals.LR_MODEL}\n",
    "], betas=(globals.ADAM_BETA1, globals.ADAM_BETA2))\n",
    "\n",
    "adam_d = torch.optim.Adam(D.parameters(), lr=globals.LR_MODEL, betas=(globals.ADAM_BETA1, globals.ADAM_BETA2))\n",
    "\n",
    "res_list = [2**i for i in range(2, int(math.log2(globals.MAX_RES))+1)]\n",
    "\n",
    "global_img_count = 0\n",
    "for res in res_list:\n",
    "    print(f\"{res}x{res} RESOLUTION\")\n",
    "    if res > 4:\n",
    "        G.fade_in(res)\n",
    "        D.fade_in(res)\n",
    "    \n",
    "    batch_sizes = {\n",
    "        4: 128,\n",
    "        8: 64,\n",
    "        16: 32,\n",
    "        32: 16\n",
    "    }\n",
    "    images_per_resolution = {\n",
    "        4: 60_000,\n",
    "        8: 120_000,\n",
    "        16: 120_000,\n",
    "        32: 180_000\n",
    "    }\n",
    "    loader = torch.utils.data.DataLoader(datasets[res], batch_size=batch_sizes[res], shuffle=True)\n",
    "\n",
    "    fade_in_imgs = int(images_per_resolution[res] * globals.FADE_IN_PERCENTAGE)\n",
    "\n",
    "    imgs_this_phase = 0\n",
    "    discriminator_steps = 0\n",
    "    while imgs_this_phase < images_per_resolution[res]:\n",
    "        print(f\"Images this phase: {imgs_this_phase}/{images_per_resolution[res]}\")\n",
    "        for real in tqdm(loader):\n",
    "            real = real.to(globals.DEVICE)\n",
    "            batch_size = real.size(0)\n",
    "\n",
    "            if imgs_this_phase < fade_in_imgs:\n",
    "                layer_opacity = min(1.0, imgs_this_phase / max(1, fade_in_imgs))\n",
    "                G.set_layer_opacity(layer_opacity)\n",
    "                D.set_layer_opacity(layer_opacity)\n",
    "            else:\n",
    "                G.set_layer_opacity(1.0)\n",
    "                D.set_layer_opacity(1.0)\n",
    "\n",
    "            for _ in range(globals.DISCRIMINATOR_STEPS):\n",
    "                # Discriminator step\n",
    "                z = torch.randn(batch_size, globals.Z_DIM, device=globals.DEVICE)\n",
    "                with torch.no_grad():\n",
    "                    fake = G(z)\n",
    "                D_loss = d_loss_non_saturating_r1(D, real, fake.detach(), d_step=discriminator_steps, ada=ada)\n",
    "                adam_d.zero_grad(set_to_none=True)\n",
    "                D_loss.backward(retain_graph=False)\n",
    "                adam_d.step()\n",
    "\n",
    "            # Generator step\n",
    "            z = torch.randn(batch_size, globals.Z_DIM, device=globals.DEVICE)\n",
    "            fake = G(z)\n",
    "            G_loss = g_loss_non_saturating(D, fake, ada)\n",
    "            adam_g.zero_grad(set_to_none=True)\n",
    "            G_loss.backward()\n",
    "            adam_g.step()\n",
    "            apply_exponential_moving_average(G, G_EMA)\n",
    "\n",
    "            imgs_this_phase += batch_size\n",
    "            global_img_count += batch_size\n",
    "\n",
    "        # visualizing output\n",
    "        z_test = torch.randn(1, globals.Z_DIM, device=globals.DEVICE)\n",
    "        fake_test = G(z_test)\n",
    "        img = (fake_test[0] + 1) / 2.0\n",
    "        img = np.transpose(img.detach().cpu().numpy(), (1, 2, 0))\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    G.set_layer_opacity(1.0)\n",
    "    D.set_layer_opacity(1.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "impressionist-stylegan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

import torch
import torch.nn as nn
import globals
import torch.nn.functional as F
from utils import EqualLRConv2d, EqualLRLinear
from ADA import ADA


class PixelNorm(nn.Module):
    def __init__(self, eps: float = 1e-8):
        super().__init__()

        self.eps = eps

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return x * torch.rsqrt(torch.mean(x * x, dim=1, keepdim=True) + self.eps)


class AdaIN(nn.Module):
    """
    Adaptive Instance Normalization.
    This is a layer of the Generator network. After we ran our original latent vector z through the
    Mapping Network and thus generated vector w, we perform a learned affine transformation on w.
    This maps w to a tuple (y_s, y_b), where "s" stands for "scale" and "b" stands for "bias".
    This tuple is then passed to an AdaIN layer, along with a feature map. The layer first
    normalizes the feature map x (the output of the convolutional layer before) to have mean=0 and sd=1 and the scales and shifts this normalized
    feature map by y_s and y_b. This controls the "style" of the output e.g. textures of features, color, etc.
    """

    def __init__(self, channels, w_dim):
        super().__init__()

        self.affine = EqualLRLinear(w_dim, channels * 2)

    def forward(self, x, w):
        """
        INPUT:
            x : (batch_size, channels, height, width)
            w : (batch_size, w_dim)
        """
        # style : (bach_size, 2*channels, 1, 1). We perform unsqueeze to use it on the feature map more easily
        style = self.affine(w).unsqueeze(-1).unsqueeze(-1)
        y_s, y_b = style.chunk(2, dim=1)
        mean = x.mean(dim=(2, 3), keepdim=True)  # mean over all pixels in each channel
        std = (
            x.var(dim=(2, 3), unbiased=False, keepdim=True).add(1e-8).sqrt()
        )  # stddev over all pixels in each channel
        x = (x - mean) / std
        return ((1 + y_s) * x) + y_b


class NoiseInjection(nn.Module):
    """
    After each convolutional layer and before the feature map is passed to the AdaIN layer,
    a single channel image consisting of noise is added to the feature map. This enables the
    Generator to be able to generate stochastic detail.
    The amount of noise that is added is different for each layer. The Generator learns during
    training how much noise it should add.
    """

    def __init__(self, channels):
        super().__init__()

        # The noise is added to the layer based on this weight parameter. At the start it is 0 --> no noise added
        self.weight = nn.Parameter(torch.zeros(1, channels, 1, 1))

    def forward(self, x, noise=None):
        if noise is None:
            noise = torch.randn(x.size(0), 1, x.size(2), x.size(3), device=x.device)
        return x + self.weight * noise


class StyledConv(nn.Module):
    """
    The main convolutional block used in the StyleGAN.
    At the beginning of the block there is an optional upsampling layer. This layer is used
    to enable the main feature of PGGANs, namely the progressive increase of resolution in the
    output image. Since not all blocks perform an upsampling operation, the layer is optional.
    Other than that the block runs the feature map through a convolutional layer and adds random noise to it,
    to enable the generator to learn stochastic details. Then the output is fed through an AdaIN layer
    to add styles to it, that have been generated by the Mapping Network.
    After that a Leaky ReLU activation function is applied.
    """

    def __init__(
        self,
        in_ch,
        out_ch,
        w_dim=globals.W_DIM,
        kernel_size=globals.KERNEL_SIZE,
        upsample=False,
    ):
        super().__init__()
        self.upsample = upsample

        self.conv = EqualLRConv2d(in_ch, out_ch, kernel_size, padding=kernel_size // 2)

        self.noise = NoiseInjection(out_ch)
        self.adain = AdaIN(out_ch, w_dim)
        self.lrelu = nn.LeakyReLU(0.2, inplace=True)

    def forward(self, x, w, noise=None):
        if self.upsample:
            x = F.interpolate(
                x, scale_factor=2, mode=globals.INTERPOLATION_MODE, align_corners=False
            )
        x = self.conv(x)
        x = self.noise(x, noise)
        x = self.adain(x, w)
        x = self.lrelu(x)
        return x


class ToRGB(nn.Module):
    """
    This block converts the learned feature maps to actual RGB values.
    For this, we apply a 1x1 convolution with 3 output channels (RGB) to the feature map.
    """

    def __init__(self, in_ch, w_dim=globals.W_DIM):
        super().__init__()

        self.conv = EqualLRConv2d(in_ch, 3, 1)

    def forward(self, x, w):
        x = self.conv(x)
        return x


def apply_exponential_moving_average(G, G_ema):
    """
    We apply this after every Generator step, to prevent weight oscillation.
    We then use the EMA Generator for evaluation purposes.
    """
    with torch.no_grad():
        decay = 0.999  # the standard decay rate

        ema_params = dict(G_ema.named_parameters())

        # we update the parameters of the EMA model by using only 1-decay of the new generator weights
        # This ensures we have smooth weight transitions in the EMA model
        for name, param in G.named_parameters():
            ema_params[name].mul_(decay).add_(param.data, alpha=1 - decay)

        # we can just copy our buffers (like layer_opacity)
        ema_buffers = dict(G_ema.named_buffers())
        for name, buffer in G.named_buffers():
            ema_buffers[name].copy_(buffer)


def g_loss_non_saturating(D, fake_imgs, ada: ADA):
    """
    The loss function of the Generator: Non-saturating logistic loss
    """
    fake_augmented = ada.augment(fake_imgs)
    fake_pred = D(fake_augmented)
    g_loss = F.softplus(-fake_pred).mean()
    return g_loss
